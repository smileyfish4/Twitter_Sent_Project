{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNQ9mYtCb3yM"
   },
   "source": [
    "-------------------------------------------------------\n",
    "# ** TWITTER SENTIMENT ANALYSIS **\n",
    "-------------------------------------------------------\n",
    "## Description\n",
    "I will be using the natural language toolkit (NLTK).\n",
    "\n",
    "In this project I will perform sentiment analysis on tweets using an approach that is based on naive Bayes. Given a tweet, I will decide if it has a positive sentiment or a negative one. In particular, we will\n",
    "\n",
    "* Train a naive Bayes model on a sentiment analysis task\n",
    "* Test our model\n",
    "* Predict on your own tweet\n",
    "\n",
    "This project will contain both the mathematical model and the necessary program to perform such tasks. The goal of this project is to teach and educate users on how to perform sentiment analysis from scratch. I will include the Step by Step approach and include explanations as per needed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis using naive Bayes\n",
    "\n",
    "Naive Bayes can be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
    "\n",
    "#### How do you train a naive Bayes classifier for sentiment analysis of tweets?\n",
    "- The first part of training a naive bayes classifier is to identify the number of classes that we have. In our case, only two: positive and negative tweets.\n",
    "- We will create a probability for each class.<br>\n",
    "$P(T_{pos})$ will be the probability that the tweet is positive.<br>\n",
    "$P(T_{neg})$ will be the probability that the tweet is negative.\n",
    "\n",
    "Assume that $T$ is the total number of, $T_{pos}$ is the total number of positive tweets, and $T_{neg}$ is the total number of negative tweets. Then\n",
    "\n",
    "$$P(T_{pos}) = \\frac{T_{pos}}{T},$$\n",
    "\n",
    "$$P(T_{neg}) = \\frac{T_{neg}}{T}.$$\n",
    "\n",
    "\n",
    "#### Prior and logprior\n",
    "\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative. In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? \n",
    "\n",
    "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "We can take the log of the prior to rescale it, and we will call this the logprior:\n",
    "\n",
    "$$\\text{logprior} = \\log \\left( \\frac{P(T_{pos})}{P(T_{neg})} \\right) = \\log \\left( \\frac{T_{pos}}{T_{neg}} \\right).$$\n",
    "\n",
    "Note that $\\log(\\frac{a}{b})$ is the same as $\\log(a) - \\log(b)$. Hence, the logprior can also be calculated as the difference between two logs:\n",
    "\n",
    "$$\\text{logprior} = \\log (P(T_{pos})) - \\log (P(T_{neg})) = \\log (T_{pos}) - \\log (T_{neg}).$$\n",
    "\n",
    "#### Positive and negative probability of a word\n",
    "To compute the positive probability and the negative probability for a specific word in the vocabulary, we will use the following inputs:\n",
    "\n",
    "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. That is, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
    "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
    "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
    "\n",
    "We will use these to compute the positive and negative probability for a specific word using this formula:\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}. $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}. $$\n",
    "\n",
    "We add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing.\n",
    "\n",
    "#### Log-likelihood\n",
    "To compute the log-likelihood of that very same word, we can implement the following equations:\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right).$$\n",
    "\n",
    "#### Sentiment score\n",
    "To compute the sentiment score of a tweet we simply add its logprior and the sum of log-likelihoods of its words:\n",
    "$$ \\text{score} = \\text{logprior} + \\sum_{i=1}^m (\\text{loglikelihood}_i),$$\n",
    "\n",
    "where $m$ is the number of words in the tweet.\n",
    "\n",
    "- If the score > 0, then the sentiment is positive.\n",
    "- If the score = 0, then the sentiment is neutral.\n",
    "- If the score < 0, then the sentiment is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "a. Implement a helper function that finds the frequency of word-label pairs.\n",
    "\n",
    "b. Train the model using naive Bayes. \n",
    "\n",
    "c. Predict the sentiment score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/rileykrisch/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rileykrisch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This question requires NLTK library. Make sure it is installed!\n",
    "# Do not modify!\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# Split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# Avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The first step is to process the data to make it useful to our model. We will do it in several steps.\n",
    "- First we will remove noise from the data. That is, we will remove words that do not tell us much about the content. For example, words like 'I, you, are, is, etc...' do not give us information on the sentiment.\n",
    "- We will remove hyperlinks, stock market tickers, retweet symbols, and hashtags. They do not contain much information on the sentiment.\n",
    "- We will remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word. We do not need to consider \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n",
    "- Finally, we want to use stemming to only keep track of one variation of each word. That is, we will treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n",
    "\n",
    "The function `process_tweet` does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    #tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'fantast', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello there! Have a fantastic day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a helper function\n",
    "\n",
    "Next we will implement `count_tweets`, a lookup helper function that takes in an empty dictionary, a word, and a label (1 or 0). The label is 1 for positive tweets and 0 is for negative tweets.\n",
    "\n",
    "The function returns a dictionary where the keys are a tuple (word, label) and the values are the corresponding frequency. That is, the dictionary should tell us the number of times that word and label tuple appears in the collection of tweets. \n",
    "\n",
    "For example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
    "\n",
    "{\n",
    "    (\"rather\", 1): 2,\n",
    "    (\"happi\", 1) : 1, \n",
    "    (\"excit\", 1) : 1\n",
    "}\n",
    "\n",
    "- Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
    "- Notice how the words \"i\" and \"am\" are not saved, because they were removed by process_tweet.\n",
    "- Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n",
    "\n",
    "Your task is to create a function `count_tweets` that takes a list of tweets as input, cleans them, and returns a dictionary.\n",
    "- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n",
    "- The value the number of times this word appears in the given collection of tweets (an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each word-label pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    # Your code\n",
    "    ### START CODE HERE ###\n",
    "    for tweet, label in zip(tweets, ys):\n",
    "        words = process_tweet(tweet)      \n",
    "        \n",
    "        for word in words:\n",
    "            pair = (word, label)  \n",
    "            if pair in result:\n",
    "                result[pair] += 1 \n",
    "            else:\n",
    "                result[pair] = 1  \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function\n",
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the `freqs` dictionary for later uses. Given your `count_tweets` function, you can compute a dictionary called `freqs` that contains all the frequencies. We will use this dictionary in several parts of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using naive Bayes\n",
    "\n",
    "Given a `freqs` dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive Bayes classifier.\n",
    "\n",
    "##### Calculate $V$\n",
    "- You can compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n",
    "\n",
    "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
    "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
    "\n",
    "##### Calculate $N_{pos}$, and $N_{neg}$\n",
    "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
    "\n",
    "##### Calculate $T$, $T_{pos}$, $T_{neg}$\n",
    "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $T$, as well as the number of positive documents (tweets) $T_{pos}$ and number of negative documents (tweets) $T_{neg}$.\n",
    "- Calculate the probability that a document (tweet) is positive $P(T_{pos})$, and the probability that a document (tweet) is negative $P(T_{neg}).$\n",
    "\n",
    "##### Calculate the logprior\n",
    "- Compute the logprior as $log(T_{pos}) - log(T_{neg}).$\n",
    "\n",
    "##### Calculate log-likelihood\n",
    "- Finally, you can iterate over each word in the vocabulary, use the provided `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
    "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using the equations below.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}, $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}.$$\n",
    "\n",
    "\n",
    "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$.\n",
    "\n",
    "**Note:** We will use a dictionary to store the log-likelihoods for each word. The key is the word, the value is the log-likelihood of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Output:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = 0  # freqs.get((word, label), 0)\n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = list(zip(*freqs.keys()))[0]\n",
    "\n",
    "    V = len(set(vocab))  \n",
    "\n",
    "    ### START YOUR CODE HERE ###\n",
    "    T_pos = sum(train_y) \n",
    "    T_neg = len(train_y) - T_pos\n",
    "    \n",
    "    N_pos = sum([freq for (word, label), freq in freqs.items() if label == 1])\n",
    "    N_neg = sum([freq for (word, label), freq in freqs.items() if label == 0])\n",
    "    \n",
    "    logprior = np.log(T_pos / T_neg)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    \n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs, word, 1.0)\n",
    "        freq_neg = lookup(freqs, word, 0.0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos+1)/(N_pos+V)\n",
    "        p_w_neg = (freq_neg+1)/(N_neg+V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        logprior = np.log(T_pos / T_neg)\n",
    "        loglikelihood[word] = np.log(p_w_pos)-np.log(p_w_neg)\n",
    "        \n",
    "\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `train_naive_bayes` function to calculate the log prior and log likelihood values, then display the log prior dictionary and the total number of word-class pairs in the log likelihood dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9161\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:<br>\n",
    "0.0<br>\n",
    "9161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the sentiment score\n",
    "\n",
    "Next step is to implement the `naive_bayes_predict` function to make predictions on tweets.\n",
    "\n",
    "The function can be described as follows:\n",
    "* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n",
    "* It returns the log of the probability that the tweet belongs to the positive or negative class.\n",
    "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
    "* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n",
    "\n",
    "$$ \\text{score} = \\text{logprior} + \\sum_{i=1}^m (\\text{loglikelihood}_i).$$\n",
    "\n",
    "Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative tweets is equal to 1. Hence, the logprior is equal to 0. This implies that we are just adding zero to the sum of log-likelihoods. However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        score: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    # Your code\n",
    "    ### START CODE HERE ###\n",
    "    score = logprior\n",
    "    words = process_tweet(tweet)\n",
    "\n",
    "    for word in words:\n",
    "        if word in loglikelihood:\n",
    "            score = score + loglikelihood[word]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1.5574928203010936\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "my_tweet = 'She smiled.'\n",
    "score = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- The expected output is around 1.55. This value implies that the sentiment is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the accuracy\n",
    "\n",
    "The below implementation of `test_naive_bayes` checks the accuracy of your predictions.\n",
    "* The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood\n",
    "* It returns the accuracy of your model.\n",
    "* First, use `naive_bayes_predict` function to make predictions for each tweet in text_x.\n",
    "\n",
    "The accuracy of the model can be computed as $$\\frac{\\text{number of tweets classified correctly}}{\\text{total number of tweets}}.$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1.0\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0.0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(abs(y_hats-test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9955\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "`Naive Bayes accuracy = 0.9955`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.14\n",
      "I am bad -> -1.31\n",
      "this movie should have been great. -> 2.12\n",
      "great -> 2.13\n",
      "great great -> 4.26\n",
      "great great great -> 6.39\n",
      "great great great great -> 8.52\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "#     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "- I am happy -> 2.14\n",
    "- I am bad -> -1.31\n",
    "- this movie should have been great. -> 2.12\n",
    "- great -> 2.13\n",
    "- great great -> 4.26\n",
    "- great great great -> 6.39\n",
    "- great great great great -> 8.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.1289430658835196)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'You are doing great!'\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Mobile_Internet_Case_Study.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
